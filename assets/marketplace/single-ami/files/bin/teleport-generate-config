#!/bin/bash

# Back up existing config file if it exists
if [ -f /etc/teleport.yaml ]; then
    rm -f /etc/teleport.yaml.old
    cp /etc/teleport.yaml /etc/teleport.yaml.old
fi

# Setup teleport config file
LOCAL_IP=$(curl -sS http://169.254.169.254/latest/meta-data/local-ipv4)
LOCAL_HOSTNAME=$(curl -sS http://169.254.169.254/latest/meta-data/local-hostname)
LOCAL_HOSTNAME=${LOCAL_HOSTNAME//./-}

# Source variables set up by cloudformation template
source /etc/teleport.d/conf

# Set host UUID so auth server picks it up, as each auth server's
# logs are stored in individual folder /var/lib/teleport/log/<host_uuid>/
# and it will be easy to log forwarders to locate them on every auth server
# note though, that host_uuid MUST be unique, otherwise all sorts of unintended
# things will happen.
echo ${LOCAL_HOSTNAME} > /var/lib/teleport/host_uuid
chown -R teleport:adm /var/lib/teleport

if [[ "${TELEPORT_ROLE}" == "auth" ]]; then
    echo "auth" > /etc/teleport.d/role.auth
    # Teleport Auth server is using DynamoDB as a backend
    # On AWS, see dynamodb.tf for details
    cat >/etc/teleport.yaml <<EOF
teleport:
  nodename: ${LOCAL_HOSTNAME}
  advertise_ip: ${LOCAL_IP}
  log:
    output: syslog
    severity: INFO

  data_dir: /var/lib/teleport
  storage:
    type: dynamodb
    region: ${EC2_REGION}
    table_name: ${TELEPORT_DYNAMO_TABLE_NAME}
    audit_table_name: ${TELEPORT_DYNAMO_EVENTS_TABLE_NAME}
    audit_sessions_uri: s3://${TELEPORT_S3_BUCKET}/records

auth_service:
  enabled: yes
  listen_addr: 0.0.0.0:3025

  authentication:
    second_factor: otp

  cluster_name: ${TELEPORT_CLUSTER_NAME}

ssh_service:
  enabled: no

proxy_service:
  enabled: no
EOF

# enable/start token services and timers
systemctl enable teleport-ssm-publish-tokens.service teleport-ssm-publish-tokens.timer
systemctl start teleport-ssm-publish-tokens.timer

# enable/start cert services and timers
systemctl enable teleport-get-cert.service teleport-get-cert.timer
systemctl start teleport-get-cert.timer

# enable auth service and disable all-in-one
systemctl disable teleport.service
systemctl enable teleport-auth.service
systemctl start teleport-auth.service --no-block

elif [[ "${TELEPORT_ROLE}" == "proxy" ]]; then
    echo "proxy" > /etc/teleport.d/role.proxy
    # Teleport proxy proxies and optionally records
    # SSH sessions
    cat >/etc/teleport.yaml <<EOF
teleport:
  auth_token: /var/lib/teleport/token
  nodename: ${LOCAL_HOSTNAME}
  advertise_ip: ${LOCAL_IP}
  log:
    output: syslog
    severity: INFO

  data_dir: /var/lib/teleport
  storage:
    type: dir
    path: /var/lib/teleport/backend
  auth_servers:
    - ${TELEPORT_AUTH_SERVER_LB}

auth_service:
  enabled: no

ssh_service:
  enabled: no

proxy_service:
  enabled: yes
  listen_addr: 0.0.0.0:3023
  tunnel_listen_addr: 0.0.0.0:3080
  web_listen_addr: 0.0.0.0:3080
  public_addr: ${TELEPORT_DOMAIN_NAME}:443
  https_cert_file: /var/lib/teleport/fullchain.pem
  https_key_file: /var/lib/teleport/privkey.pem
EOF

# enable proxy service and disable all-in-one
systemctl disable teleport.service
systemctl enable teleport-proxy.service
systemctl start teleport-proxy.service --no-block

elif [[ "${TELEPORT_ROLE}" == "node" ]]; then
    echo "node" > /etc/teleport.d/role.node
    # Teleport node handles incoming connections
    cat >/etc/teleport.yaml <<EOF
teleport:
  auth_token: /var/lib/teleport/token
  nodename: ${LOCAL_HOSTNAME}
  advertise_ip: ${LOCAL_IP}
  log:
    output: syslog
    severity: INFO

  data_dir: /var/lib/teleport
  storage:
    type: dir
    path: /var/lib/teleport/backend
  auth_servers:
    - ${TELEPORT_AUTH_SERVER_LB}

auth_service:
  enabled: no

ssh_service:
  enabled: yes
  listen_addr: 0.0.0.0:3022

proxy_service:
  enabled: no
EOF

# enable node service and disable all-in-one
systemctl disable teleport.service
systemctl enable teleport-node.service
systemctl start teleport-node.service --no-block

elif [[ "${TELEPORT_ROLE}" == "monitor" ]]; then
    echo "monitor" > /etc/teleport.d/role.monitor
    # disable teleport service if this has the monitor role
    systemctl disable teleport.service
    # no teleport config needed
    rm -f /etc/teleport.yaml
    # run monitor setup as an independent service
    systemctl start teleport-monitor-setup.service --no-block

else
    echo "No Teleport role provided via TELEPORT_ROLE; using all-in-one config"
    echo "all" > /etc/teleport.d/role.all
    chmod +x /etc/teleport.d/role.all
    cat >/etc/teleport.yaml <<EOF
teleport:
  nodename: ${LOCAL_HOSTNAME}
  advertise_ip: ${LOCAL_IP}
  log:
    output: syslog
    severity: INFO

  data_dir: /var/lib/teleport
  storage:
    type: dir
    path: /var/lib/teleport/backend

auth_service:
  enabled: yes
  listen_addr: 0.0.0.0:3025

  authentication:
    second_factor: otp

ssh_service:
  enabled: yes
  listen_addr: 0.0.0.0:3022

proxy_service:
  enabled: yes
  listen_addr: 0.0.0.0:3023
  tunnel_listen_addr: 0.0.0.0:3080
  web_listen_addr: 0.0.0.0:3080
  public_addr: ${TELEPORT_EXTERNAL_HOSTNAME:-$LOCAL_HOSTNAME}:${TELEPORT_EXTERNAL_PORT:-3025}
EOF
fi

# enable telegraf service if running in cluster mode
if [[ "${TELEPORT_ROLE}" == "auth" || "${TELEPORT_ROLE}" == "node" || "${TELEPORT_ROLE}" == "proxy" ]]; then
    # Install teleport telegraf configuration
    # Telegraf will collect prometheus metrics and send to influxdb collector
    cat >/etc/telegraf/telegraf.conf <<EOF
# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at
  ## most metric_batch_size metrics.
  metric_batch_size = 1000
  ## For failed writes, telegraf will cache metric_buffer_limit metrics for each
  ## output, and will flush this buffer on a successful write. Oldest metrics
  ## are dropped first when this buffer fills.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. You shouldn't set this below
  ## interval. Maximum flush_interval will be flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default, precision will be set to the same timestamp order as the
  ## collection interval, with the maximum being 1s.
  ## Precision will NOT be used for service inputs, such as logparser and statsd.
  precision = ""
  ## Run telegraf in debug mode
  debug = false
  ## Run telegraf in quiet mode
  quiet = false
  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false

###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

[[inputs.procstat]]
  exe = "teleport"
  prefix = "teleport"

[[inputs.prometheus]]
  # An array of urls to scrape metrics from.
  urls = ["http://127.0.0.1:3434/metrics"]
  # Add a metric name prefix
  name_prefix = "teleport_"
  # Add tags to be able to make beautiful dashboards
  [inputs.prometheus.tags]
    teleservice = "teleport"

# Read metrics about cpu usage
[[inputs.cpu]]
  ## Whether to report per-cpu stats or not
  percpu = true
  ## Whether to report total system cpu stats or not
  totalcpu = true
  ## If true, collect raw CPU time metrics.
  collect_cpu_time = false
  ## If true, compute and report the sum of all non-idle CPU states.
  report_active = false

# Read metrics about disk usage by mount point
[[inputs.disk]]
  ## By default, telegraf gather stats for all mountpoints.
  ## Setting mountpoints will restrict the stats to the specified mountpoints.
  # mount_points = ["/"]

  ## Ignore some mountpoints by filesystem type. For example (dev)tmpfs (usually
  ## present on /run, /var/run, /dev/shm or /dev).
  ignore_fs = ["tmpfs", "devtmpfs", "devfs"]

# Read metrics about disk IO by device
[[inputs.diskio]]

# Get kernel statistics from /proc/stat
[[inputs.kernel]]
  # no configuration

# Read metrics about memory usage
[[inputs.mem]]
  # no configuration

# Get the number of processes and group them by status
[[inputs.processes]]
  # no configuration

# Read metrics about swap memory usage
[[inputs.swap]]
  # no configuration

# Read metrics about system load & uptime
[[inputs.system]]
  # no configuration

###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

# Configuration for influxdb server to send metrics to
[[outputs.influxdb]]
  ## The full HTTP or UDP endpoint URL for your InfluxDB instance.
  ## Multiple urls can be specified as part of the same cluster,
  ## this means that only ONE of the urls will be written to each interval.
  urls = ["${TELEPORT_INFLUXDB_ADDRESS}"] # required
  ## The target database for metrics (telegraf will create it if not exists).
  database = "telegraf" # required

  ## Retention policy to write to. Empty string writes to the default rp.
  retention_policy = ""
  ## Write consistency (clusters only), can be: "any", "one", "quorum", "all"
  write_consistency = "any"

  ## Write timeout (for the InfluxDB client), formatted as a string.
  ## If not provided, will default to 5s. 0s means no timeout (not recommended).
  timeout = "5s"
EOF
    systemctl enable telegraf.service
    systemctl restart telegraf.service
fi